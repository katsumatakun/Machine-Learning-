{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'skdist' from 'C:\\\\Users\\\\masahiro\\\\Anaconda3\\\\lib\\\\site-packages\\\\skdist\\\\__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import skdist\n",
    "skdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|preds|              scores|\n",
      "+-----+--------------------+\n",
      "|    0|[0.99988133364581...|\n",
      "|    1|[4.75036320495255...|\n",
      "|    2|[2.94891479564409...|\n",
      "|    3|[1.63439184184317...|\n",
      "|    4|[1.11340782802743...|\n",
      "|    5|[1.47167847402628...|\n",
      "|    6|[1.08555580162344...|\n",
      "|    7|[3.02428696147597...|\n",
      "|    8|[7.65455387782709...|\n",
      "|    9|[3.97697358804967...|\n",
      "|    0|[0.99919579687825...|\n",
      "|    1|[2.65210459891626...|\n",
      "|    2|[1.85886626332130...|\n",
      "|    3|[2.89824549681289...|\n",
      "|    4|[2.84814929909155...|\n",
      "|    5|[2.70091084586477...|\n",
      "|    6|[1.10907939716380...|\n",
      "|    7|[3.06454485209046...|\n",
      "|    8|[2.38830589881912...|\n",
      "|    9|[8.24574106207122...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+--------------------+\n",
      "|preds|              scores|\n",
      "+-----+--------------------+\n",
      "|    4|[0.03736128393565...|\n",
      "|    0|[0.09792807410478...|\n",
      "|   17|[0.05044543817914...|\n",
      "|   11|[0.03443972986074...|\n",
      "|   10|[0.04757471929521...|\n",
      "|   15|[0.04555477151025...|\n",
      "|    4|[0.04025302976824...|\n",
      "|   17|[0.04606538206124...|\n",
      "|    4|[0.05296440750891...|\n",
      "|   12|[0.04526243345294...|\n",
      "|    4|[0.03733198188990...|\n",
      "|    6|[0.04041213769366...|\n",
      "|    4|[0.04252566904405...|\n",
      "|   15|[0.04738860601686...|\n",
      "|    4|[0.03942044494467...|\n",
      "|   11|[0.04281835124858...|\n",
      "|   11|[0.03675331309090...|\n",
      "|    4|[0.03287753061778...|\n",
      "|   12|[0.04517622045917...|\n",
      "|   11|[0.04878195327579...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+--------------------+\n",
      "|preds|              scores|\n",
      "+-----+--------------------+\n",
      "|    4|[0.03736128393565...|\n",
      "|    0|[0.09792807410478...|\n",
      "|   17|[0.05044543817914...|\n",
      "|   11|[0.03443972986074...|\n",
      "|   10|[0.04757471929521...|\n",
      "|   15|[0.04555477151025...|\n",
      "|    4|[0.04025302976824...|\n",
      "|   17|[0.04606538206124...|\n",
      "|    4|[0.05296440750891...|\n",
      "|   12|[0.04526243345294...|\n",
      "|    4|[0.03733198188990...|\n",
      "|    6|[0.04041213769366...|\n",
      "|    4|[0.04252566904405...|\n",
      "|   15|[0.04738860601686...|\n",
      "|    4|[0.03942044494467...|\n",
      "|   11|[0.04281835124858...|\n",
      "|   11|[0.03675331309090...|\n",
      "|    4|[0.03287753061778...|\n",
      "|   12|[0.04517622045917...|\n",
      "|   11|[0.04878195327579...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_20newsgroups, load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skdist.distribute.predict import get_prediction_udf\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# spark session initialization\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .getOrCreate()\n",
    "    )\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# simple 2-D numpy features\n",
    "data = load_digits()\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"]\n",
    "model = LogisticRegression(\n",
    "    solver=\"liblinear\", \n",
    "    multi_class=\"auto\"\n",
    "    )\n",
    "model.fit(X, y)\n",
    "\n",
    "# get UDFs with default 'numpy' feature types\n",
    "predict = get_prediction_udf(model, method=\"predict\")\n",
    "predict_proba = get_prediction_udf(model, method=\"predict_proba\")\n",
    "\n",
    "# create PySpark DataFrame from features\n",
    "pdf = pd.DataFrame(X)\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "cols = [F.col(str(c)) for c in sdf.columns]\n",
    "\n",
    "# apply predict UDFs and select prediction output\n",
    "prediction_df = (\n",
    "    sdf\n",
    "    .withColumn(\"scores\", predict_proba(*cols))\n",
    "    .withColumn(\"preds\", predict(*cols))\n",
    "    .select(\"preds\", \"scores\")\n",
    "    )\n",
    "prediction_df.show()\n",
    "\n",
    "# single text feature \n",
    "data = fetch_20newsgroups(\n",
    "    shuffle=True, random_state=1,\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    "    )\n",
    "X = data[\"data\"][:100]\n",
    "y = data[\"target\"][:100]\n",
    "model = Pipeline([\n",
    "    (\"vec\", HashingVectorizer()), \n",
    "    (\"clf\", LogisticRegression(solver=\"liblinear\", multi_class=\"auto\"))\n",
    "    ])\n",
    "model.fit(X, y)\n",
    "\n",
    "# get UDFs with 'text' feature types\n",
    "predict = get_prediction_udf(model, method=\"predict\", feature_type=\"text\")\n",
    "predict_proba = get_prediction_udf(model, method=\"predict_proba\", feature_type=\"text\")\n",
    "\n",
    "# create PySpark DataFrame from features\n",
    "pdf = pd.DataFrame(X)\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "cols = [F.col(str(c)) for c in sdf.columns]\n",
    "\n",
    "# apply predict UDFs and select prediction output\n",
    "prediction_df = (\n",
    "    sdf\n",
    "    .withColumn(\"scores\", predict_proba(*cols))\n",
    "    .withColumn(\"preds\", predict(*cols))\n",
    "    .select(\"preds\", \"scores\")\n",
    "    )\n",
    "prediction_df.show()\n",
    "\n",
    "# complex feature space as pandas DataFrame\n",
    "X = pd.DataFrame({\"text\": data[\"data\"][:100]})\n",
    "y = data[\"target\"][:100]\n",
    "model = Pipeline([\n",
    "    (\"vec\", ColumnTransformer([(\"text\", HashingVectorizer(), \"text\")])), \n",
    "    (\"clf\", LogisticRegression(solver=\"liblinear\", multi_class=\"auto\"))\n",
    "    ])\n",
    "model.fit(X, y)\n",
    "\n",
    "# get UDFs with 'pandas' feature types\n",
    "# NOTE: This time we must supply an ordered list\n",
    "# of column names to the `get_predict_udf` function\n",
    "predict = get_prediction_udf(model, method=\"predict\", feature_type=\"pandas\", names=list(X.columns))\n",
    "predict_proba = get_prediction_udf(model, method=\"predict_proba\", feature_type=\"pandas\", names=list(X.columns))\n",
    "\n",
    "# create PySpark DataFrame from features\n",
    "sdf = spark.createDataFrame(X)\n",
    "cols = [F.col(str(c)) for c in sdf.columns]\n",
    "\n",
    "# apply predict UDFs and select prediction output\n",
    "prediction_df = (\n",
    "    sdf\n",
    "    .withColumn(\"scores\", predict_proba(*cols))\n",
    "    .withColumn(\"preds\", predict(*cols))\n",
    "    .select(\"preds\", \"scores\")\n",
    "    )\n",
    "prediction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
